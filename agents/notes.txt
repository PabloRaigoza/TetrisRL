M1_BC10000.dat - 10000 epochs on AgentM1, BCELoss, argmax action selection
M1_BC20000.dat - 20000 epochs on AgentM1, BCELoss, argmax action selection
M2_BC10000.dat - 10000 epochs on AgentM2, BCELoss, argmax action selection
M2_BC20000.dat - 20000 epochs on AgentM2, CE Loss, categorical action selection

Current Issues/Work To Do:
- Tune the architecture or hyperparameters to achieve a better BC performance.
- Training DAgger agent for at least 10 datasets.
- Possibly use another expert policy for BC or DAgger instead of ourselves.

- Right now REINFORCE dominates any starting policy, figure out how to limit this.
- REINFORCE achieves 0 training loss quickly, but doesn't get good results.
- Basic exploration term for REINFORCE is given, but we need to mess with it more.

- Maybe give more importance to the last move that gives a reward.
- Change the reward function to incentivize survival and line clears.


Reserach Online Models
- I found three online models that had various implementations of good Tetris bots
- 

- We realized that reinforce was varrying widly from the intial starting conditition of BC
- In order to fix this, we decided to start with our BC model, then have REINFORCE perform the exploration
- We will allow reinforce to learn a little, then we'll average our the parameters of the model to create a new model
- We will start learning again with this model, and continue. This will force the model to be similar to BC

1. Reinfroce dominated
2. Tried, fixed it using averaging technique 
3. 